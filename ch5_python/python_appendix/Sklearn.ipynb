{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Modeling Procedure\n",
    "### Scikit-learn requries Python >=3.6\n",
    "## Step 1: Data import \n",
    "### You can load the data from sklearn.datasets API. The dataset loader is for small and Toy dataset, while the dataset fetcher is for huge and Real World dataset. Here are the examples to load small data and fetch large data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()  \n",
    "faces = datasets.fetch_olivetti_faces()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also have a function to download the data, this is to keep tack of the up-to-date data and allows for automation of dara fetching process. Below presented the code to fetch zip file from web and extract all .csv files to your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import urllib\n",
    "\n",
    "Root_Download = './master/..'\n",
    "url_download = Root_Download +'Data.zip'\n",
    "Data_Path = os.path.join ('Data')\n",
    "\n",
    "def fetch_Data (url = url_download, Path =Data_Path):\n",
    "    os.makedirs(Path, exist_ok=True)\n",
    "    zip_path = os.path.join (Path, 'Dataset.zip')\n",
    "    filepath, _ = urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip:\n",
    "        zip.extractall(Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have dataset downloaded, let us write a function to load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def data_loader (data_path, data):\n",
    "    csv_path = os.path.join (data_path, data)\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next I will walk you through preprocessing using one of our dataset, COVID_19.csv, and use head(10) to see the first 10 rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   ID  Age Sex            RS Travel?     CC     DC        tmp  COVID?\n0   1   44   M  Non-Canadian    True   True  False  37.464557    True\n1   2   16   M  Non-Canadian    True   True   True  39.282884    True\n2   3   36   M      Canadian    True  False   True  35.839426   False\n3   4   53   F  Non-Canadian   False   True   True  38.117380    True\n4   5   11   F      Canadian   False  False   True  36.933678   False\n5   6   12   M  Non-Canadian   False  False   True  35.772355   False\n6   7   18   F      Canadian   False  False  False        NaN   False\n7   8   56   F  Non-Canadian    True  False  False  37.070684    True\n8   9    7   M      Canadian    True   True  False  36.924761    True\n9  10   16   M  Non-Canadian    True  False  False  37.445650    True",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>RS</th>\n      <th>Travel?</th>\n      <th>CC</th>\n      <th>DC</th>\n      <th>tmp</th>\n      <th>COVID?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>44</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>37.464557</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>16</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>39.282884</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>36</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>35.839426</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>53</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>38.117380</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>11</td>\n      <td>F</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>36.933678</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>12</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>35.772355</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>18</td>\n      <td>F</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>56</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>37.070684</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>7</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>36.924761</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>16</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>37.445650</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "os.chdir('/Users/rachelzeng/dsbook')\n",
    "\n",
    "import pandas as pd\n",
    "Data_Path =os.path.join ('Data')\n",
    "data_name = 'COVID_19_preprocess.csv'\n",
    "COVID = data_loader (data_path=Data_Path, data= data_name)\n",
    "COVID_copy = COVID.copy()\n",
    "COVID.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a small dataset features 200 rows and 9 columns, notice that it is a simulated data for better ilustrate, thus no real meaning apply to the result.\n",
    "\n",
    "#### Suppose that this data is collected from COVID 19 Assessment Center, it listed 200 suspected patient waiting for testing.\n",
    "\n",
    "#### Next, there are some useful methods that provide quick desription of the data. dataset.info() will show you number and name of the columns with associate data type.\n",
    "\n",
    "#### You will find that this dataset contain 200 instances, 9 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 9 columns):\nID         200 non-null int64\nAge        200 non-null int64\nSex        200 non-null object\nRS         200 non-null object\nTravel?    199 non-null object\nCC         199 non-null object\nDC         199 non-null object\ntmp        199 non-null float64\nCOVID?     200 non-null bool\ndtypes: bool(1), float64(1), int64(2), object(5)\nmemory usage: 12.8+ KB\nNone\n"
    }
   ],
   "source": [
    "print(COVID.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The describe() method is showing the summary of statistics of each attributes, such as mean, standard deviation, min, and max. Knowing them you will have a general idea of how does the distirbution of each attribute look like. Notice that when you use describe() method, it only return the summary of statistics for Age and Temperature attributes, it is becuase describe() only works for numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ID         Age         tmp\ncount  200.000000  200.000000  199.000000\nmean   100.500000   42.085000   37.634978\nstd     57.879185   22.869694    1.315427\nmin      1.000000    4.000000   35.008780\n25%     50.750000   22.000000   36.699820\n50%    100.500000   42.500000   37.720827\n75%    150.250000   62.250000   38.652300\nmax    200.000000   80.000000   39.878757\n"
    }
   ],
   "source": [
    "print(COVID.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for a minute, so what Problem, exactly, are we solving here? For a given project, the very first step is actually to understand the purpose of the study and what is optimal result return to the user in assisting them to make decision?\n",
    "\n",
    "### Suppose we are interested in predicting whether a suspected patient is infected with COVID 19. In order to make a robust model, we need to ensure our model can perform well on the new data. Assume we have a set of data, then this set should be divided into training and testing data, training data is what need to be fed into and testing data is what the model has not seen, and needed to be tested. We will talk more about it in Appendix for Machine Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   ID  Age Sex            RS Travel?     CC     DC        tmp\n0   1   44   M  Non-Canadian    True   True  False  37.464557\n1   2   16   M  Non-Canadian    True   True   True  39.282884\n2   3   36   M      Canadian    True  False   True  35.839426\n3   4   53   F  Non-Canadian   False   True   True  38.117380\n4   5   11   F      Canadian   False  False   True  36.933678",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>RS</th>\n      <th>Travel?</th>\n      <th>CC</th>\n      <th>DC</th>\n      <th>tmp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>44</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>37.464557</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>16</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>39.282884</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>36</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>35.839426</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>53</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>38.117380</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>11</td>\n      <td>F</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>36.933678</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "target_name = 'COVID?'\n",
    "target = COVID.pop (target_name)\n",
    "COVID.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 160 entries, 108 to 37\nData columns (total 8 columns):\nID         160 non-null int64\nAge        160 non-null int64\nSex        160 non-null object\nRS         160 non-null object\nTravel?    159 non-null object\nCC         159 non-null object\nDC         159 non-null object\ntmp        159 non-null float64\ndtypes: float64(1), int64(2), object(5)\nmemory usage: 11.2+ KB\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_Data, Test_Data, target_train, target_test = train_test_split(COVID, target, test_size=0.2, random_state =1)\n",
    "Train_Data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We simply use train_test_split function from Sklearn to do random spliting. test_size = 0.2 indicates the proportion of testing samples, ie. if we have 100 samples, then data set has 20 samples. random_state allows you to set random gnerator seed, ie. this will garurantee to get same result each time you run the experiment. (otherwise put 0 or nothing.) \n",
    "\n",
    "### Using the random smapling like the one above is fine if we have a large dataset, but since we are working with only 200 samples, we need to ensure that the samples selected for training data is representative of the whole. A better way is to conduct stratified sampling. \n",
    "\n",
    "### Stratify is a way to maintain the distirbution of pre-split classes. For example, suppose we have 100 suspected patients, 80 suspected patients are infected, 20 are not. If we need 75 training samples, in order to preserve the distribution, we must have 60 infected and 15 uninfected in the sample. (Similar to testing data) \n",
    "\n",
    "### Let us see how to do this using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 160 entries, 18 to 90\nData columns (total 8 columns):\nID         160 non-null int64\nAge        160 non-null int64\nSex        160 non-null object\nRS         160 non-null object\nTravel?    159 non-null object\nCC         159 non-null object\nDC         159 non-null object\ntmp        159 non-null float64\ndtypes: float64(1), int64(2), object(5)\nmemory usage: 11.2+ KB\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_Data, Test_Data, target_train, target_test  = train_test_split(COVID,target,test_size=0.2, random_state=1,stratify=target)\n",
    "Train_Data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with Missing Values\n",
    "### The very first step is to clean our data, if we use the Sklearn datasets like Iris, Boston housing price etc., these datasets are high quality with no missing values. However, often time we need to deal with dirty data, it contain missing values, noise; Specially when we are dealing with clinical dataset, inaccurate data due to measurement error, and needs to deal with variety of data types and sources. Low quality of dara will cause us great trouble in the later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Travel?    1\nCC         1\nDC         1\ntmp        1\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "Null_col = COVID.columns[ COVID.isnull().any()]\n",
    "COVID[Null_col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      ID  Age Sex            RS Travel?     CC     DC        tmp\n6      7   18   F      Canadian   False  False  False        NaN\n100  101   56   F      Canadian     NaN   True  False  36.672816\n103  104   39   F  Non-Canadian   False    NaN  False  37.135664\n108  109   74   M      Canadian    True   True    NaN  36.683842",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>RS</th>\n      <th>Travel?</th>\n      <th>CC</th>\n      <th>DC</th>\n      <th>tmp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>18</td>\n      <td>F</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>101</td>\n      <td>56</td>\n      <td>F</td>\n      <td>Canadian</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>False</td>\n      <td>36.672816</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>104</td>\n      <td>39</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>37.135664</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>109</td>\n      <td>74</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>36.683842</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "COVID.loc[pd.isnull(COVID).any(axis = 1),:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a list of index of rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[  6 100 103 108]\n"
    }
   ],
   "source": [
    "Null_row = COVID.loc[pd.isnull(COVID).any(axis = 1),:].index.values # return index of rows with NaN in dataframe\n",
    "print(Null_row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are severeal ways to deal with missing values. The simplest way is to remove the entire sample or variable from the data depending how many missing values in a sample or a variable(attribute). If it contians more than a certian percentage of missing value, then delete the missing entries or the entire column. In our case, we do not have too much missing value for the attribues, so we can delete the missing entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  However, discard entire rows and/or columns may comes at the expense of potentially valuable data, and left with fewer samples. A better way to deal with numerical attribute, like Temperature, is by inferring them from the data. \\textit{Sklearn.SimpleImputer} provides strategies to estimate the missing values, such as fill the NaN with mean, medium or most frequent of the column where missing value is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID['tmp']=COVID['tmp'].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [ID, Age, Sex, RS, Travel?, CC, DC, tmp]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>RS</th>\n      <th>Travel?</th>\n      <th>CC</th>\n      <th>DC</th>\n      <th>tmp</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "Mean_imp = SimpleImputer(missing_values= np.nan, strategy = 'mean')\n",
    "COVID['tmp'] = Mean_imp.fit_transform (COVID['tmp'].values.reshape(-1,1)) \n",
    "# .reshape(1,-1) indicate transform to array with 1 row, and do not care about how many column. \n",
    "COVID[COVID['tmp'].isnull()] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We replece NaN values in Temperature with the mean, and drop rows which contain missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      ID  Age Sex                  RS Travel?     CC     DC        tmp\n0      1   44   M        Non-Canadian    True   True  False  37.464557\n1      2   16   M        Non-Canadian    True   True   True  39.282884\n2      3   36   M            Canadian    True  False   True  35.839426\n3      4   53   F        Non-Canadian   False   True   True  38.117380\n4      5   11   F            Canadian   False  False   True  36.933678\n..   ...  ...  ..                 ...     ...    ...    ...        ...\n195  196   51   F        Non-Canadian    True   True   True  37.905418\n196  197   39   M            Canadian   False  False  False  35.301528\n197  198   68   M            Canadian   False  False  False  38.035950\n198  199   75   F        Non-Canadian    True   True  False  39.422919\n199  200   45   M  Permanent Resident    True   True  False  37.900000\n\n[197 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>RS</th>\n      <th>Travel?</th>\n      <th>CC</th>\n      <th>DC</th>\n      <th>tmp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>44</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>37.464557</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>16</td>\n      <td>M</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>39.282884</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>36</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>35.839426</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>53</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>38.117380</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>11</td>\n      <td>F</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>36.933678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>196</td>\n      <td>51</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>37.905418</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>197</td>\n      <td>39</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>35.301528</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>198</td>\n      <td>68</td>\n      <td>M</td>\n      <td>Canadian</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>38.035950</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>199</td>\n      <td>75</td>\n      <td>F</td>\n      <td>Non-Canadian</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>39.422919</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>200</td>\n      <td>45</td>\n      <td>M</td>\n      <td>Permanent Resident</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>37.900000</td>\n    </tr>\n  </tbody>\n</table>\n<p>197 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "row_null = COVID.loc[pd.isnull(COVID).any(axis = 1),:].index.values\n",
    "COVID = COVID.drop(row_null)\n",
    "COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(['Travel?', 'CC', 'DC'], ['ID', 'Age', 'tmp'], ['Sex', 'RS'])"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "COVID[['Travel?', 'CC', 'DC']] = COVID[['Travel?', 'CC', 'DC']].astype('bool')\n",
    "COVID[['RS',  'Sex']] = COVID[['RS', 'Sex']].astype('category')\n",
    "Bool_attribute = list(list(COVID.select_dtypes(include=['bool']).columns))\n",
    "Num_attribute = list(COVID.select_dtypes(include=['number']).columns)\n",
    "Cat_attribute = list(COVID.select_dtypes(include=['category']).columns)\n",
    "Bool_attribute, Num_attribute,Cat_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to run numerical analyses, we will need to convert values from categorical to numerical by assigning muerical codes to them. As we can see column: Residency Status, and Sex are categorical data type . The column: Travel? Close contact, Dry Vough and COVID_19? are boolean, which means it only returns True or False. \n",
    "\n",
    "### There are two ways to encode numerical data to numbers, LabelEncoder() or OneHotEncoder(). So what are the differences? \n",
    "\n",
    "##  LabelEncoder() vs. OneHotEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 3, 0, 1, 0,\n       1, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 3,\n       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 0, 0, 3, 0, 0, 0, 1, 0, 1, 2, 0, 3, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 1, 2,\n       1, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       1, 2, 0, 1, 1, 0, 3, 3, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 3, 2, 1, 2, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2])"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "label_encoded = label_enc.fit_transform(COVID['RS'])\n",
    "label_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Canadian', 'Refugee', 'Refugee', 'Non-Canadain', 'Non-Canadain', 'Non-Canadain', 'Permanent Resident', 'Refugee']\n"
    }
   ],
   "source": [
    "# generate random integer values\n",
    "from random import seed\n",
    "from random import sample\n",
    "import random\n",
    "seed(1)\n",
    "Status = ['Canadian','Non-Canadain', 'Permanent Resident','Refugee']\n",
    "Random_residency = random.choices(Status, k=197)\n",
    "print(Random_residency[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 3, 3, 1, 1, 1, 2, 3])"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_enc = LabelEncoder()\n",
    "label_encoded_res = label_enc.fit_transform(Random_residency)\n",
    "label_encoded_res[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Suppose we lable encode the Residency Status, we will get the result as seen above.  As you can see 0 = Canadian, 1 = Non-Canadian, 2 = Permanent Resident, 3 = Refugee. However, this is problematic because the model will misunderstand the data to be numerical order, like $0<1<2<3$. For example, it is not the case that as Residency Status number increase, their age increase. In fact, what we actually want is to use numerical values to replace unrelated categorical data. That is why OneHot Encoder is used. OneHot Encoder on the other hand will first splits the column into multiple columns with distinct Residency Status, and depends on which column has what value, it will replace with 0 or 1. To see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1. 0. 0. 0.]\n [0. 0. 0. 1.]\n [0. 0. 0. 1.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n"
    }
   ],
   "source": [
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OH_enc = OneHotEncoder()\n",
    "Resi_col = pd.DataFrame(Random_residency)\n",
    "Resi_encoded = OH_enc.fit_transform(Resi_col).toarray()\n",
    "print(Resi_encoded[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             RS\n0  Non-Canadian\n1  Non-Canadian\n2      Canadian\n3  Non-Canadian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Non-Canadian</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Non-Canadian</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Canadian</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Non-Canadian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# Method 2：Using Sklearn Preprocessing\n",
    "Resident_cat = COVID[['RS']]\n",
    "Resident_cat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array(['Canadian', 'Non-Canadian', 'Permanent Resident', 'Refugee'],\n       dtype=object),\n array([1, 2, 3, 4], dtype=object)]"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OH_enc = OneHotEncoder()\n",
    "label = [['Canadian' , 1], ['Non-Canadian',2], ['Permanent Resident', 3], ['Refugee',4]]\n",
    "OH_enc.fit(label)\n",
    "OH_enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<197x4 sparse matrix of type '<class 'numpy.float64'>'\n\twith 197 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "Residency_encoded = OH_enc.fit_transform(Resident_cat)\n",
    "Residency_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 0., 0., 1.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.]])"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "Residency_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_train = Train_Data.loc[pd.isnull(Train_Data).any(axis = 1),:].index.values\n",
    "Train_Data = Train_Data.drop(null_train)\n",
    "null_test = Test_Data.loc[pd.isnull(Test_Data).any(axis = 1),:].index.values\n",
    "Test_Data = Test_Data.drop(null_test)\n",
    "\n",
    "\n",
    "target_train= target_train.drop(null_train)\n",
    "target_test = target_test.drop(null_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Pipeline\n",
    "### A very useful technique in Sklearn is the Pipeline class. In general, Pipeline groups a set of activates together to a perform task, this can manage and fixed the sequence of all steps, in which makes it easy to reuse parameter sets on new data. In the data prepossessing, let us build a pipeline with sequences of \"fit and transform\", to deal with each distinct data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer\n",
    "### The additional flexibility provided to data preprocessing from the \\textit{FunctionTransromer} function, this allows you to build you own transformers to clean and organize the data. You should know by now that we have been constantly using \\textit{fit()}, \\textit{transform()} or \\textit{fittransform()}, this is the basic usage of most of Sklearn functions. Later, you will get to know other features, such as in regression has \\textit{coef} that store the regression coefficients and \\textit{intercept\\_} is to store intercept.\n",
    "\n",
    "### Similarly, building a custom transformer also need to consist \\textit{fit()} and \\textit{transform()}, let us build a pipeline for our target\\_train and target\\_test data processing, such pipeline includes two simple transformer classes that drop the rows where at least one element is missing and convert Boolean values into either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n       1, 0])"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "class drop_na(object):\n",
    "    def __init__(self,attribute_name=True):\n",
    "        self.attribute_name = attribute_name\n",
    "  \n",
    "    def transform(self, X):\n",
    "        X_cp = X.copy()\n",
    "        #null = np.argwhere(np.isnan(X.cp))\n",
    "        X_cp = X_cp[~np.isnan(X_cp).any(axis=1)]\n",
    "        return X_cp \n",
    "     \n",
    "    def fit (self, X_cp, y=None):\n",
    "        return self\n",
    "\n",
    "class target_encode(object):\n",
    "    def __init__(self, attribute_name=True):\n",
    "        self.attribute_name = attribute_name\n",
    "  \n",
    "    def transform(self, X):\n",
    "        X_cp = X.copy()\n",
    "        X_cp = np.where (X_cp == True, 1, X_cp)\n",
    "        return X_cp\n",
    "    def fit (self, X_cp, y=None):\n",
    "        return self\n",
    "\n",
    "cat_pip = Pipeline([\n",
    "    ('imputer_na', drop_na()), # Handle missing data \n",
    "    ('imputer_Bool', target_encode()),\n",
    "])\n",
    "\n",
    "target_train = cat_pip.transform(np.asarray(target_train).reshape(1,-1))\n",
    "target_test = cat_pip.transform(np.asarray(target_test).reshape(1,-1))\n",
    "target_train = target_train[0]\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 1, 1, 0, 1, 1, 1, 0])"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "target_train[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick comment on three common data preprocessing tools: \n",
    "### StandardScaler: The data transofrmed into a standard normal distribution with a mean of 0 and a variance of 1 (Z-score).\n",
    "### RobustScaler: Similar to StandardScaler. Instead of using mean and variance, it uses the median and quartile. Therefore, this will get rid of the outliers directly.\n",
    "### MinMaxScaler: Simple method to shift and scale the data so that the values returned within the range of 0 and 1, we can do this by subract the min value and divide by the the difference between max and min values.\n",
    "### Normalizer: It will first convert the samples into Euclidean distance of 1, the data distribution thus turn into a circle with a radius of 1. It is used when we are interested in only the distance and direction of the data not the value itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer # helps perform different transformations for di\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_pip = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy= 'mean')), # Handle missing data \n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(['Travel?', 'CC', 'DC'], ['ID', 'Age', 'tmp'], ['Sex', 'RS'])"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "Bool_attribute, Num_attribute,Cat_attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-1.47893671, -0.47426715,  0.99996833, ...,  0.        ,\n         0.        ,  1.        ],\n       [ 1.70919033,  1.42961319,  1.32933101, ...,  1.        ,\n         0.        ,  0.        ],\n       [ 0.00885591,  0.45553488, -0.00245309, ...,  0.        ,\n         0.        ,  1.        ],\n       ...,\n       [-0.27453316, -1.18268774, -1.64452875, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.49664852, -1.35979289,  1.27059076, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.20368589,  0.45553488, -1.29768869, ...,  1.        ,\n         0.        ,  0.        ]])"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "cat = Bool_attribute + Cat_attribute\n",
    "Full_pip = ColumnTransformer([\n",
    "    ('num_age_temp' , numerical_pip, Num_attribute),\n",
    "    ('cat_Bool', OneHotEncoder(), cat)\n",
    "])\n",
    "\n",
    "Trained_transformed = Full_pip.fit_transform(Train_Data)\n",
    "Tested_transformed = Full_pip.fit_transform(Test_Data)\n",
    "Trained_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition, pipeline can be also used to chain classification estimator, but do keep in mind that estimator should comes after all the transfromers.\n",
    "###  Pipline vs. featureUnion: featureUnion is another way to chian transformer objects into one, different from Pipeline, the transformers are applied in parallel while Pipeline execute the transformers in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "### Finally, enough for the suffer. Here comes the most exicting part, let us build a model to predict whether a suspected patient is infected with COVID 19. Sklearn provides packages for supervised(labeled target) and unsupervised learning(unlabeled target),  these models are called \"Estimator\" which is used to do prediction or regression. In general the estimators will also have the following functions: \n",
    "### 1. fit() : fed in attribute data and target to train the model, other parameters like batch size, learning rate etc. On the other hand, fit() in preprcoessing can be used to caculates mean and variance, and accept data trasnformation method. \n",
    "### 2. predict() : used for data prediction, fed in the input data and it wll return a prediction labels in numpy array. We usually feed the testing set into predict() and then compare with the true test labels.\n",
    "### 3. score() : used to calculate the accuracy of the model, thus it range between 0 and 1. Notice that this is the most basic indicator to evaluate the performance of the model, there are other indicators such as recall rate or precision rate. Under certain circumstances, having one indicator is not enough to judge whether a model is a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit() and predict() with LogisticRegression.\n",
    "### We also import classification_report, this will return a more comprehensive report of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.63      0.63      0.63        19\n           1       0.67      0.67      0.67        21\n\n    accuracy                           0.65        40\n   macro avg       0.65      0.65      0.65        40\nweighted avg       0.65      0.65      0.65        40\n\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-278f6da46f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTested_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classification = LogisticRegression(solver = 'lbfgs' , multi_class= 'multinomial')\n",
    "classification.fit(Trained_transformed, target_train)\n",
    "predict = classification.predict(Tested_transformed)\n",
    "print(classification_report(target_test[0], predict))\n",
    "print(accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Understand Classification _report include: F1, recall rate, and precision rate.  Give a visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use joblib to save our model\n",
    "\n",
    "### Note: it will save directly to your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\rache\\\\Downloads\\\\dsbook\\\\Saved_models'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-3af2ff41dfd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\rache\\\\Downloads\\\\dsbook\\\\Saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\rache\\\\Downloads\\\\dsbook\\\\Saved_models'"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\rache\\\\Downloads\\\\dsbook\\\\Saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['sk_classification.pkl']"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump (classification, 'sk_classification.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reuse_clf = joblib.load ( 'sk_classification.pkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The test score is 65.00 %\n"
    }
   ],
   "source": [
    "score = Reuse_clf.score(Tested_transformed,target_test[0])\n",
    "print(\"The test score is {0:.2f} %\".format(100 * score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not suprisingly, using Logistic Regression with all attribute columns resulted in poor rate of predictive score. We will revisit this problem in Machine Learning section with additional knowledge on the subject of classification and random sampling, such as feature selections, bootstraping and implement random forest classifier to improve the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Travel?', 'CC', 'DC', 'Sex', 'RS']"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = ['Age','Temperature','Travel Y', 'Travel N', 'Close contact Y', 'Close contact N', 'Residency Status- Permanent ', 'Residency Status-NC ','Residency Status- C','Residency Status- R','Education level-U','Education level-H','Education level-E',  'Dry Cough Y','Dry Cough- N', 'Sex-M', 'Sex-F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Length of feature_names, 17 does not match number of features, 15",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-9e0714ad07ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'infected'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'noninfected'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                      \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                      special_characters=True)  \n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrounded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial_characters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             precision=precision)\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mexporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, decision_tree)\u001b[0m\n\u001b[1;32m    397\u001b[0m                                  \u001b[0;34m\"does not match number of features, %d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                                  % (len(self.feature_names),\n\u001b[0;32m--> 399\u001b[0;31m                                     decision_tree.n_features_))\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;31m# each part writes to out_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of feature_names, 17 does not match number of features, 15"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "clf.fit(Trained_transformed, target_train)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=col_name, \n",
    "                     class_names=['infected', 'noninfected'] ,\n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.format = 'png'\n",
    "graph.render(\"COVID 19 Prediction\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('Age', 0.20614240050838217) ('Temperature', 0.16864286219048735) ('Travel Y', 0.5092179612274076) ('Travel N', 0.0) ('Close contact Y', 0.012875524114222534) ('Close contact N', 0.0) ('Residency Status- Permanent ', 0.02063072240560612) ('Residency Status-NC ', 0.030610834347260543) ('Residency Status- C', 0.014977370827421267) ('Residency Status- R', 0.010528779737709703) ('Education level-U', 0.0) ('Education level-H', 0.026373544641502752) ('Education level-E', 0.0) ('Dry Cough Y', 0.0) ('Dry Cough- N', 0.0)\n"
    }
   ],
   "source": [
    "print(*zip(col_name ,clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Length of feature_names, 17 does not match number of features, 15",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-f77d2aa3ca25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'infected'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'noninfected'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                      \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                      special_characters=True)  \n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgraph2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrounded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial_characters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             precision=precision)\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mexporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/tree/_export.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, decision_tree)\u001b[0m\n\u001b[1;32m    397\u001b[0m                                  \u001b[0;34m\"does not match number of features, %d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                                  % (len(self.feature_names),\n\u001b[0;32m--> 399\u001b[0;31m                                     decision_tree.n_features_))\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;31m# each part writes to out_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of feature_names, 17 does not match number of features, 15"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "clf2 = tree.DecisionTreeClassifier(criterion = 'entropy', splitter= 'random')\n",
    "clf2.fit(Trained_transformed, target_train)\n",
    "dot_data = tree.export_graphviz(clf2, out_file=None, \n",
    "                     feature_names=col_name, \n",
    "                     class_names=['infected', 'noninfected'] ,\n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph2 = graphviz.Source(dot_data)  \n",
    "graph.format = 'png'\n",
    "graph2.render(\"COVID 19 Prediction_random\") \n",
    "graph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}